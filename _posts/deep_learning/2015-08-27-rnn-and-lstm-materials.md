---
layout: post
categories: deep_learning
title: RNN and LSTM Materials
---

{{ page.title }}
================

<p class="meta">27 Aug 2015 - Beijing</p>

**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**

- paper: [http://arxiv.org/abs/1502.03044](http://arxiv.org/abs/1502.03044)
- code: [https://github.com/kelvinxu/arctic-captions](https://github.com/kelvinxu/arctic-captions)

**Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets**

- paper: [http://arxiv.org/abs/1503.01007](http://arxiv.org/abs/1503.01007)
- code: [https://github.com/facebook/Stack-RNN](https://github.com/facebook/Stack-RNN)

**Recurrent Models of Visual Attention** (Google DeepMind. NIPS2014)

- paper: [http://arxiv.org/abs/1406.6247](http://arxiv.org/abs/1406.6247)
- data: [https://github.com/deepmind/mnist-cluttered](https://github.com/deepmind/mnist-cluttered)
- code: [https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua](https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua)

**Depth-Gated LSTM**

- paper: [http://arxiv.org/abs/1508.03790](http://arxiv.org/abs/1508.03790)
- code: [GitHub(dglstm.h+dglstm.cc)](https://github.com/kaishengyao/cnn/tree/master/cnn)

**Unsupervised Learning of Video Representations using LSTMs(ICML2015)**

- project: [http://www.cs.toronto.edu/~nitish/unsupervised_video/](http://www.cs.toronto.edu/~nitish/unsupervised_video/)
- paper: [http://arxiv.org/abs/1502.04681](http://arxiv.org/abs/1502.04681)
- code: [http://www.cs.toronto.edu/~nitish/unsupervised_video/unsup_video_lstm.tar.gz](http://www.cs.toronto.edu/~nitish/unsupervised_video/unsup_video_lstm.tar.gz)

**Visualizing and Understanding Recurrent Networks(Andrej Karpathy, Justin Johnson, Fei-Fei Li)**

- paper: [http://arxiv.org/abs/1506.02078](http://arxiv.org/abs/1506.02078)

**Project: pycaffe-recurrent**

- code: [https://github.com/kuprel/pycaffe-recurrent/](https://github.com/kuprel/pycaffe-recurrent/)

**Recurrent Model of Visual Attention(Google DeepMind)**

- paper: [http://arxiv.org/abs/1406.6247](http://arxiv.org/abs/1406.6247)
- GitXiv: [http://gitxiv.com/posts/ZEobCXSh23DE8a8mo/recurrent-models-of-visual-attention](http://gitxiv.com/posts/ZEobCXSh23DE8a8mo/recurrent-models-of-visual-attention)
- blog: [http://torch.ch/blog/2015/09/21/rmva.html](http://torch.ch/blog/2015/09/21/rmva.html)
- code: [https://github.com/Element-Research/rnn/blob/master/scripts/evaluate-rva.lua](https://github.com/Element-Research/rnn/blob/master/scripts/evaluate-rva.lua)

**LSTM: A Search Space Odyssey**

- paper: [http://arxiv.org/abs/1503.04069](http://arxiv.org/abs/1503.04069)
- notes: "Notes on LSTM: A Search Space Odyssey" by Hugo Larochelle    <br />
[https://www.evernote.com/shard/s189/sh/48da42c5-8106-4f0d-b835-c203466bfac4/50d7a3c9a961aefd937fae3eebc6f540](https://www.evernote.com/shard/s189/sh/48da42c5-8106-4f0d-b835-c203466bfac4/50d7a3c9a961aefd937fae3eebc6f540)

**A Critical Review of Recurrent Neural Networks for Sequence Learning**

- arXiv: [http://arxiv.org/abs/1506.00019](http://arxiv.org/abs/1506.00019)
- intro: "A rigorous & readable review on RNNs"  <br /> [http://blog.terminal.com/a-thorough-and-readable-review-on-rnns/](http://blog.terminal.com/a-thorough-and-readable-review-on-rnns/)

**Jointly Modeling Embedding and Translation to Bridge Video and Language**

- arXiv: [http://arxiv.org/abs/1505.01861](http://arxiv.org/abs/1505.01861)
