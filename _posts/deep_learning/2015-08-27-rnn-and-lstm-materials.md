---
layout: post
categories: deep_learning
title: RNN and LSTM Materials
---

{{ page.title }}
================

<p class="meta">27 Aug 2015 - Beijing</p>

**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**

- paper: [arXiv:1502.03044](http://arxiv.org/abs/1502.03044)
- code: [https://github.com/kelvinxu/arctic-captions](https://github.com/kelvinxu/arctic-captions)

**Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets**

- paper: [arXiv:1503.01007](http://arxiv.org/abs/1503.01007)
- code: [https://github.com/facebook/Stack-RNN](https://github.com/facebook/Stack-RNN)

**Recurrent Models of Visual Attention** (Google DeepMind. NIPS2014)

- paper: [http://arxiv.org/abs/1406.6247](http://arxiv.org/abs/1406.6247)
- data: [https://github.com/deepmind/mnist-cluttered](https://github.com/deepmind/mnist-cluttered)
- code: [https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua](https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua)

**Depth-Gated LSTM**

- paper: [http://arxiv.org/abs/1508.03790](http://arxiv.org/abs/1508.03790)
- code: [GitHub(dglstm.h+dglstm.cc)](https://github.com/kaishengyao/cnn/tree/master/cnn)

**Unsupervised Learning of Video Representations using LSTMs(ICML2015)**

- project: [http://www.cs.toronto.edu/~nitish/unsupervised_video/](http://www.cs.toronto.edu/~nitish/unsupervised_video/)
- paper: [http://arxiv.org/abs/1502.04681](http://arxiv.org/abs/1502.04681)
- code: [http://www.cs.toronto.edu/~nitish/unsupervised_video/unsup_video_lstm.tar.gz](http://www.cs.toronto.edu/~nitish/unsupervised_video/unsup_video_lstm.tar.gz)

**Visualizing and Understanding Recurrent Networks(Andrej Karpathy, Justin Johnson, Fei-Fei Li)**

- paper: [http://arxiv.org/abs/1506.02078](http://arxiv.org/abs/1506.02078)

**Project: pycaffe-recurrent**

- code: [https://github.com/kuprel/pycaffe-recurrent/](https://github.com/kuprel/pycaffe-recurrent/)
