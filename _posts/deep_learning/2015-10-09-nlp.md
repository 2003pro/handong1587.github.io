---
layout: post
category: deep_learning
title: NLP
date: 2015-10-09
---

* TOC
{:toc}

# Tutorials

![](/assets/deep_learning/NLP/Deep Learning For NLP.jpg)

# Neural Models

**Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models**

<img src="/assets/dl-materials/Unifying_Visual-Semantic_Embeddings_with_Multimodal_Neural_Language_Models.png"/>

- paper: [http://arxiv.org/abs/1411.2539](http://arxiv.org/abs/1411.2539)
- results: [http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html](http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html)
- demo: [http://deeplearning.cs.toronto.edu/i2t](http://deeplearning.cs.toronto.edu/i2t)
- github: [https://github.com/ryankiros/visual-semantic-embedding](https://github.com/ryankiros/visual-semantic-embedding)

**Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks**

- arXiv: [http://arxiv.org/abs/1503.00075](http://arxiv.org/abs/1503.00075)
- github: [https://github.com/stanfordnlp/treelstm](https://github.com/stanfordnlp/treelstm)

**Visualizing and Understanding Neural Models in NLP**

- arXiv: [http://arxiv.org/abs/1506.01066](http://arxiv.org/abs/1506.01066)

**Character-Aware Neural Language Models**

- paper: [http://arxiv.org/abs/1508.06615](http://arxiv.org/abs/1508.06615)
- github: [https://github.com/yoonkim/lstm-char-cnn](https://github.com/yoonkim/lstm-char-cnn)

**Skip-Thought Vectors**

- paper: [http://arxiv.org/abs/1506.06726](http://arxiv.org/abs/1506.06726)
- github: [https://github.com/ryankiros/skip-thoughts](https://github.com/ryankiros/skip-thoughts)

**A Primer on Neural Network Models for Natural Language Processing**

- arXiv: [http://arxiv.org/abs/1510.00726](http://arxiv.org/abs/1510.00726)

**Character-aware Neural Language Models**

- arxiv: [http://arxiv.org/abs/1508.06615](http://arxiv.org/abs/1508.06615)

**Neural Variational Inference for Text Processing**

- arxiv: [http://arxiv.org/abs/1511.06038](http://arxiv.org/abs/1511.06038)
- notes: [http://dustintran.com/blog/neural-variational-inference-for-text-processing/](http://dustintran.com/blog/neural-variational-inference-for-text-processing/)
- github: [https://github.com/carpedm20/variational-text-tensorflow](https://github.com/carpedm20/variational-text-tensorflow)
- github: [https://github.com/cheng6076/NVDM](https://github.com/cheng6076/NVDM)

# Sequence to Sequence Learning

**Generating Text with Deep Reinforcement Learning(NIPS 2015)**

- arXiv: [http://arxiv.org/abs/1510.09202](http://arxiv.org/abs/1510.09202)

**MUSIO: A Deep Learning based Chatbot Getting Smarter**

- homepage: [http://ec2-204-236-149-143.us-west-1.compute.amazonaws.com:9000/](http://ec2-204-236-149-143.us-west-1.compute.amazonaws.com:9000/)
- github(Torch7): [https://github.com/deepcoord/seq2seq](https://github.com/deepcoord/seq2seq)

# Translation

**Learning phrase representations using rnn encoder-decoder for statistical machine translation**

- arXiv: [http://arxiv.org/abs/1406.1078](http://arxiv.org/abs/1406.1078)

**Neural Machine Translation by Jointly Learning to Align and Translate**

- arXiv: [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)
- github: [https://github.com/lisa-groundhog/GroundHog](https://github.com/lisa-groundhog/GroundHog)

**Multi-Source Neural Translation**

- intro: "report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model."
- arxiv: [Multi-Source Neural Translation](Multi-Source Neural Translation)
- github(Zoph_RNN): [https://github.com/isi-nlp/Zoph_RNN](https://github.com/isi-nlp/Zoph_RNN)
- video: [http://research.microsoft.com/apps/video/default.aspx?id=260336](http://research.microsoft.com/apps/video/default.aspx?id=260336)

**Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism**

- arxiv: [http://arxiv.org/abs/1601.01073](http://arxiv.org/abs/1601.01073)

**A Character-level Decoder without Explicit Segmentation for Neural Machine Translation**

- arxiv: [http://arxiv.org/abs/1603.06147](http://arxiv.org/abs/1603.06147)
- github: [https://github.com/nyu-dl/dl4mt-cdec](https://github.com/nyu-dl/dl4mt-cdec)

**NEMATUS: Attention-based encoder-decoder model for neural machine translation**

- github: [https://github.com/rsennrich/nematus](https://github.com/rsennrich/nematus)

# Summarization

**Extraction of Salient Sentences from Labelled Documents**

- arxiv: [http://arxiv.org/abs/1412.6815](http://arxiv.org/abs/1412.6815)
- github: [https://github.com/mdenil/txtnets](https://github.com/mdenil/txtnets)
- notes: [https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2014/06/model-visualizing-summarising-conv-net.md](https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2014/06/model-visualizing-summarising-conv-net.md)

**A Neural Attention Model for Abstractive Sentence Summarization(EMNLP 2015. Facebook AI Research)**

- arXiv: [http://arxiv.org/abs/1509.00685](http://arxiv.org/abs/1509.00685)
- github: [https://github.com/facebook/NAMAS](https://github.com/facebook/NAMAS)

**A Convolutional Attention Network for Extreme Summarization of Source Code**

![](https://camo.githubusercontent.com/95dfe4b12b966b664fd441b19430405520a859a9/687474703a2f2f7333322e706f7374696d672e6f72672f7263326664793079642f53637265656e5f53686f745f323031365f30355f30395f61745f31305f31385f33365f504d2e706e67)

- homepage: [http://groups.inf.ed.ac.uk/cup/codeattention/](http://groups.inf.ed.ac.uk/cup/codeattention/)
- arxiv: [http://arxiv.org/abs/1602.03001](http://arxiv.org/abs/1602.03001)
- github: [https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2016/02/conv-attention-network-source-code-summarization.md](https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2016/02/conv-attention-network-source-code-summarization.md)

# Question Answering

**Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks(Facebook AI Research)**

- arXiv: [http://arxiv.org/abs/1502.05698v1](http://arxiv.org/abs/1502.05698v1)
- github: [https://github.com/facebook/bAbI-tasks](https://github.com/facebook/bAbI-tasks)

**VQA: Visual Question Answering**

- arxiv: [http://arxiv.org/abs/1505.00468](http://arxiv.org/abs/1505.00468)
- homepage: [http://visualqa.org/](http://visualqa.org/)

**Ask Your Neurons: A Neural-based Approach to Answering Questions about Images (ICCV 2015)**

- arxiv: [http://arxiv.org/abs/1505.01121](http://arxiv.org/abs/1505.01121)
- project: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/)
- video: [https://www.youtube.com/watch?v=QZEwDcN8ehs&hd=1](https://www.youtube.com/watch?v=QZEwDcN8ehs&hd=1)

**Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering**

- arxiv: [http://arxiv.org/abs/1505.05612](http://arxiv.org/abs/1505.05612)

**Teaching Machines to Read and Comprehend(Google DeepMind)**

- arXiv: [http://arxiv.org/abs/1506.03340](http://arxiv.org/abs/1506.03340)
- github: [https://github.com/deepmind/rc-data](https://github.com/deepmind/rc-data)
- github(Theano/Blocks): [https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend](https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend)
- github(Tensorflow): [https://github.com/carpedm20/attentive-reader-tensorflow](https://github.com/carpedm20/attentive-reader-tensorflow)

**Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction**

![](http://cvlab.postech.ac.kr/research/dppnet/images/figure2.png)

- arxiv: [http://arxiv.org/abs/1511.05756](http://arxiv.org/abs/1511.05756)
- github: [https://github.com/HyeonwooNoh/DPPnet](https://github.com/HyeonwooNoh/DPPnet)
- project page: [http://cvlab.postech.ac.kr/research/dppnet/](http://cvlab.postech.ac.kr/research/dppnet/)

**Neural Generative Question Answering**

- arXiv: [http://arxiv.org/abs/1512.01337](http://arxiv.org/abs/1512.01337)

**Simple Baseline for Visual Question Answering (Facebook AI Research. Bag-of-word)**

- arXiv: [http://arxiv.org/abs/1512.02167](http://arxiv.org/abs/1512.02167)
- github: [https://github.com/metalbubble/VQAbaseline](https://github.com/metalbubble/VQAbaseline)
- demo: [http://visualqa.csail.mit.edu/](http://visualqa.csail.mit.edu/)

**MovieQA: Understanding Stories in Movies through Question-Answering**

- arxiv: [http://arxiv.org/abs/1512.02902](http://arxiv.org/abs/1512.02902)
- homepage: [http://movieqa.cs.toronto.edu/home/](http://movieqa.cs.toronto.edu/home/)

**Deeper LSTM+ normalized CNN for Visual Question Answering**

- intro: "This current code can get 58.16 on Open-Ended and 63.09 on Multiple-Choice on test-standard split"
- github: [https://github.com/VT-vision-lab/VQA_LSTM_CNN](https://github.com/VT-vision-lab/VQA_LSTM_CNN)

**A Neural Network for Factoid Question Answering over Paragraphs**

- project page: [http://cs.umd.edu/~miyyer/qblearn/](http://cs.umd.edu/~miyyer/qblearn/)
- paper: [https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf](https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf)
- code+data: [https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz](https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz)

**Generating Natural Questions About an Image**

- arxiv: [http://arxiv.org/abs/1603.06059](http://arxiv.org/abs/1603.06059)

**Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus**

- arxiv: [http://arxiv.org/abs/1603.06807](http://arxiv.org/abs/1603.06807)

**Character-Level Question Answering with Attention**

- arxiv: [http://arxiv.org/abs/1604.00727](http://arxiv.org/abs/1604.00727)
- comment(by @Wenpeng_Yin): "fancy model with minor improvement"

**A Focused Dynamic Attention Model for Visual Question Answering**

- arxiv: [http://arxiv.org/abs/1604.01485](http://arxiv.org/abs/1604.01485)

**Visual Question Answering Literature Survey**

- blog: [http://iamaaditya.github.io/research/literature/](http://iamaaditya.github.io/research/literature/)

**The DIY Guide to Visual Question Answering**

![](https://camo.githubusercontent.com/53c28e13bd645acbf49c9e71e82a36202d1981bc/687474703a2f2f7333322e706f7374696d672e6f72672f77636a6c7a7a7532742f53637265656e5f53686f745f323031365f30355f30385f61745f325f34325f30375f504d2e706e67)

- github: [https://github.com/jxieeducation/DIY-Data-Science/blob/master/research/visual_qa.md](https://github.com/jxieeducation/DIY-Data-Science/blob/master/research/visual_qa.md)

**Question Answering via Integer Programming over Semi-Structured Knowledge**

- arxiv: [http://arxiv.org/abs/1604.06076](http://arxiv.org/abs/1604.06076)
- github: [https://github.com/allenai/tableilp](https://github.com/allenai/tableilp)
- youtube: [https://www.youtube.com/watch?v=7NS53icQRrs](https://www.youtube.com/watch?v=7NS53icQRrs)

## Language Understanding

**Recurrent Neural Networks with External Memory for Language Understanding**

- arxiv: [http://arxiv.org/abs/1506.00195](http://arxiv.org/abs/1506.00195)
- github: [https://github.com/npow/RNN-EM](https://github.com/npow/RNN-EM)

## Projects

**VQA Demo: Visual Question Answering Demo on pretrained model**

- github: [https://github.com/iamaaditya/VQA_Demo](https://github.com/iamaaditya/VQA_Demo)
- ref: [http://iamaaditya.github.io/research/](http://iamaaditya.github.io/research/)

**TheanoLM - An Extensible Toolkit for Neural Network Language Modeling**

- arxiv: [http://arxiv.org/abs/1605.00942](http://arxiv.org/abs/1605.00942)
- github: [https://github.com/senarvi/theanolm](https://github.com/senarvi/theanolm)

**Sentence Convolution Code in Torch: Text classification using a convolutional neural network**

- github: [https://github.com/harvardnlp/sent-conv-torch](https://github.com/harvardnlp/sent-conv-torch)

**NLP-Caffe: natural language processing with Caffe**

- github: [https://github.com/Russell91/nlpcaffe](https://github.com/Russell91/nlpcaffe)

## Dataset

**Visual7W: Grounded Question Answering in Images**

![](http://web.stanford.edu/~yukez/images/img/visual7w_examples.png)

- homepage: [http://web.stanford.edu/~yukez/visual7w/](http://web.stanford.edu/~yukez/visual7w/)
- github: [https://github.com/yukezhu/visual7w-toolkit](https://github.com/yukezhu/visual7w-toolkit)
- github: [https://github.com/yukezhu/visual7w-qa-models](https://github.com/yukezhu/visual7w-qa-models)

# Alignment

**Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books**

- arXiv: [http://arxiv.org/abs/1506.06724](http://arxiv.org/abs/1506.06724)
- github: [https://github.com/ryankiros/neural-storyteller](https://github.com/ryankiros/neural-storyteller)

# Papers

**Globally Normalized Transition-Based Neural Networks**

![](https://raw.githubusercontent.com/tensorflow/models/master/syntaxnet/looping-parser.gif)

- intro: speech tagging, dependency parsing and sentence compression 
- arxiv: [http://arxiv.org/abs/1603.06042](http://arxiv.org/abs/1603.06042)
- github(SyntaxNet): [https://github.com/tensorflow/models/tree/master/syntaxnet](https://github.com/tensorflow/models/tree/master/syntaxnet)

# Demos

**AskImage.org - Deep Learning for Answering Questions about Images**

- homepage: [http://www.askimage.org/](http://www.askimage.org/)

# Resources

**So, you need to understand language data? Open-source NLP software can help!**

![](http://entopix.com/assets/white-paper/slide1.png)

- blog: [http://entopix.com/so-you-need-to-understand-language-data-open-source-nlp-software-can-help.html](http://entopix.com/so-you-need-to-understand-language-data-open-source-nlp-software-can-help.html)