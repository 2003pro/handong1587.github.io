---
layout: post
category: deep_learning
title: NLP
date: 2015-10-09
---

* TOC
{:toc}

# Neural Models

**Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models**

<img src="/assets/dl-materials/Unifying_Visual-Semantic_Embeddings_with_Multimodal_Neural_Language_Models.png"/>

- paper: [http://arxiv.org/abs/1411.2539](http://arxiv.org/abs/1411.2539)
- results: [http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html](http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html)
- demo: [http://deeplearning.cs.toronto.edu/i2t](http://deeplearning.cs.toronto.edu/i2t)
- github: [https://github.com/ryankiros/visual-semantic-embedding](https://github.com/ryankiros/visual-semantic-embedding)

**Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks**

- arXiv: [http://arxiv.org/abs/1503.00075](http://arxiv.org/abs/1503.00075)
- github: [https://github.com/stanfordnlp/treelstm](https://github.com/stanfordnlp/treelstm)

**Visualizing and Understanding Neural Models in NLP**

- arXiv: [http://arxiv.org/abs/1506.01066](http://arxiv.org/abs/1506.01066)

**Character-Aware Neural Language Models**

- paper: [http://arxiv.org/abs/1508.06615](http://arxiv.org/abs/1508.06615)
- github: [https://github.com/yoonkim/lstm-char-cnn](https://github.com/yoonkim/lstm-char-cnn)

**Skip-Thought Vectors**

- paper: [http://arxiv.org/abs/1506.06726](http://arxiv.org/abs/1506.06726)
- github: [https://github.com/ryankiros/skip-thoughts](https://github.com/ryankiros/skip-thoughts)

**A Primer on Neural Network Models for Natural Language Processing**

- arXiv: [http://arxiv.org/abs/1510.00726](http://arxiv.org/abs/1510.00726)

**Character-aware Neural Language Models**

- arxiv: [http://arxiv.org/abs/1508.06615](http://arxiv.org/abs/1508.06615)

# Sequence to Sequence Learning

**Generating Text with Deep Reinforcement Learning(NIPS 2015)**

- arXiv: [http://arxiv.org/abs/1510.09202](http://arxiv.org/abs/1510.09202)

# Translation

**Neural Machine Translation by Jointly Learning to Align and Translate**

- arXiv: [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)
- github: [https://github.com/lisa-groundhog/GroundHog](https://github.com/lisa-groundhog/GroundHog)

**Multi-Source Neural Translation**

- intro: "report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model."
- arxiv: [Multi-Source Neural Translation](Multi-Source Neural Translation)
- github(Zoph_RNN): [https://github.com/isi-nlp/Zoph_RNN](https://github.com/isi-nlp/Zoph_RNN)

**Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism**

- arxiv: [http://arxiv.org/abs/1601.01073](http://arxiv.org/abs/1601.01073)

# Summarization

**A Neural Attention Model for Abstractive Sentence Summarization(EMNLP 2015. Facebook AI Research)**

- arXiv: [http://arxiv.org/abs/1509.00685](http://arxiv.org/abs/1509.00685)
- github: [https://github.com/facebook/NAMAS](https://github.com/facebook/NAMAS)

# Question Answering

**Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks(Facebook AI Research)**

- arXiv: [http://arxiv.org/abs/1502.05698v1](http://arxiv.org/abs/1502.05698v1)
- github: [https://github.com/facebook/bAbI-tasks](https://github.com/facebook/bAbI-tasks)

**Teaching Machines to Read and Comprehend(Google DeepMind)**

- arXiv: [http://arxiv.org/abs/1506.03340](http://arxiv.org/abs/1506.03340)
- github: [https://github.com/deepmind/rc-data](https://github.com/deepmind/rc-data)

**Neural Generative Question Answering**

- arXiv: [http://arxiv.org/abs/1512.01337](http://arxiv.org/abs/1512.01337)

**Simple Baseline for Visual Question Answering (Facebook AI Research. Bag-of-word)**

- arXiv: [http://arxiv.org/abs/1512.02167](http://arxiv.org/abs/1512.02167)
- github: [https://github.com/metalbubble/VQAbaseline](https://github.com/metalbubble/VQAbaseline)
- demo: [http://visualqa.csail.mit.edu/](http://visualqa.csail.mit.edu/)

**MovieQA: Understanding Stories in Movies through Question-Answering**

- arxiv: [http://arxiv.org/abs/1512.02902](http://arxiv.org/abs/1512.02902)
- homepage: [http://movieqa.cs.toronto.edu/home/](http://movieqa.cs.toronto.edu/home/)

**Deeper LSTM+ normalized CNN for Visual Question Answering**

- intro: "This current code can get 58.16 on Open-Ended and 63.09 on Multiple-Choice on test-standard split"
- github: [https://github.com/VT-vision-lab/VQA_LSTM_CNN](https://github.com/VT-vision-lab/VQA_LSTM_CNN)

# Alignment

**Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books**

- arXiv: [http://arxiv.org/abs/1506.06724](http://arxiv.org/abs/1506.06724)
- github: [https://github.com/ryankiros/neural-storyteller](https://github.com/ryankiros/neural-storyteller)