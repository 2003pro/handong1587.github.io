---
layout: post
category: deep_learning
title: Attention Models
date: 2015-10-16
---

# Paper

**A Neural Attention Model for Abstractive Sentence Summarization(EMNLP 2015. Facebook AI Research)**

- arXiv: [http://arxiv.org/abs/1509.00685](http://arxiv.org/abs/1509.00685)
- github: [https://github.com/facebook/NAMAS](https://github.com/facebook/NAMAS)

**Effective Approaches to Attention-based Neural Machine Translation(EMNLP2015)**

- paper: [http://nlp.stanford.edu/pubs/emnlp15_attn.pdf](http://nlp.stanford.edu/pubs/emnlp15_attn.pdf)
- project: [http://nlp.stanford.edu/projects/nmt/](http://nlp.stanford.edu/projects/nmt/)

# Blog

**Survey on Attention-based Models Applied in NLP**

[http://yanran.li/peppypapers/2015/10/07/survey-attention-model-1.html](http://yanran.li/peppypapers/2015/10/07/survey-attention-model-1.html)

**Survey on Advanced Attention-based Models**

[http://yanran.li/peppypapers/2015/10/07/survey-attention-model-2.html](http://yanran.li/peppypapers/2015/10/07/survey-attention-model-2.html)

"干货 | Applied Attention-Based Models (二）"

[http://mp.weixin.qq.com/s?__biz=MzAwMjM3MTc5OA==&mid=212530824&idx=1&sn=55ca55ddb7ce7d2cd9b316c598b6da03](http://mp.weixin.qq.com/s?__biz=MzAwMjM3MTc5OA==&mid=212530824&idx=1&sn=55ca55ddb7ce7d2cd9b316c598b6da03)

"干货 | 都是 Character-level/Aware Language Models 差距咋就这么大捏"

[http://mp.weixin.qq.com/s?__biz=MzAwMjM3MTc5OA==&mid=212942444&idx=1&sn=3c3bdc25f7fab1090145f3d8604d76c5](http://mp.weixin.qq.com/s?__biz=MzAwMjM3MTc5OA==&mid=212942444&idx=1&sn=3c3bdc25f7fab1090145f3d8604d76c5)
