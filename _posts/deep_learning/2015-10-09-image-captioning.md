---
layout: post
category: deep_learning
title: Image Captioning
date: 2015-10-09
---

**Optimizing Neural Networks That Generate Images(2014. PhD thesis)**

- paper : [http://www.cs.toronto.edu/~tijmen/tijmen_thesis.pdf](http://www.cs.toronto.edu/~tijmen/tijmen_thesis.pdf)
- github: [https://github.com/mrkulk/Unsupervised-Capsule-Network](https://github.com/mrkulk/Unsupervised-Capsule-Network)

**Show and Tell: A Neural Image Caption Generator(Google)**

- arXiv: [http://arxiv.org/abs/1411.4555](http://arxiv.org/abs/1411.4555)
- github: [https://github.com/karpathy/neuraltalk](https://github.com/karpathy/neuraltalk)
- GitXiv: [http://gitxiv.com/posts/7nofxjoYBXga5XjtL/show-and-tell-a-neural-image-caption-nic-generator](http://gitxiv.com/posts/7nofxjoYBXga5XjtL/show-and-tell-a-neural-image-caption-nic-generator)
- github: [https://github.com/apple2373/chainer_caption_generation](https://github.com/apple2373/chainer_caption_generation)
- blog("Image caption generation by CNN and LSTM"): [http://t-satoshi.blogspot.com/2015/12/image-caption-generation-by-cnn-and-lstm.html](http://t-satoshi.blogspot.com/2015/12/image-caption-generation-by-cnn-and-lstm.html)

**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**

- arXiv: [http://arxiv.org/abs/1502.03044](http://arxiv.org/abs/1502.03044)
- github: [https://github.com/kelvinxu/arctic-captions](https://github.com/kelvinxu/arctic-captions)

**Describing Videos by Exploiting Temporal Structure**

- arXiv: [http://arxiv.org/abs/1502.08029](http://arxiv.org/abs/1502.08029)
- github: [https://github.com/yaoli/arctic-capgen-vid](https://github.com/yaoli/arctic-capgen-vid)

**Sequence to Sequence -- Video to Text(ICCV 2015)**

![](/assets/image_captioning/S2VTarchitecture.png)

- arXiv: [http://arxiv.org/abs/1505.00487](http://arxiv.org/abs/1505.00487)
- project: [http://vsubhashini.github.io/s2vt.html](http://vsubhashini.github.io/s2vt.html)
- github: [https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt)
- github: [https://github.com/jazzsaxmafia/video_to_sequence](https://github.com/jazzsaxmafia/video_to_sequence)

**Learning FRAME Models Using CNN Filters for Knowledge Visualization(CVPR 2015)**

- arXiv: [http://arxiv.org/abs/1509.08379](http://arxiv.org/abs/1509.08379)
- project page: [http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html](http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html)
- code+data: [http://www.stat.ucla.edu/~yang.lu/project/deepFrame/doc/deepFRAME_1.1.zip](http://www.stat.ucla.edu/~yang.lu/project/deepFrame/doc/deepFRAME_1.1.zip)

**Generating Images from Captions with Attention**

- arXiv: [http://arxiv.org/abs/1511.02793](http://arxiv.org/abs/1511.02793)
- demo: [http://www.cs.toronto.edu/~emansim/cap2im.html](http://www.cs.toronto.edu/~emansim/cap2im.html)

**Order-Embeddings of Images and Language**

- arXiv: [http://arxiv.org/abs/1511.06361](http://arxiv.org/abs/1511.06361)
- github: [https://github.com/ivendrov/order-embedding](https://github.com/ivendrov/order-embedding)

**DenseCap: Fully Convolutional Localization Networks for Dense Captioning**

- arXiv: [http://arxiv.org/abs/1511.07571](http://arxiv.org/abs/1511.07571)

**Expressing an Image Stream with a Sequence of Natural Sentences (CRCN. NIPS 2015)**

![](/assets/image_captioning/stream2text_nips.jpg)

- nips-page: [http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences](http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences)
- paper: [http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf](http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf)
- paper: [http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf](http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf)
- author-page: [http://www.cs.cmu.edu/~gunhee/](http://www.cs.cmu.edu/~gunhee/)
- github: [https://github.com/cesc-park/CRCN](https://github.com/cesc-park/CRCN)